#!/bin/bash
#SBATCH --job-name=fed_client
#SBATCH --mail-type=ALL
#SBATCH --mail-user=your_mail
#SBATCH --account=your account
#SBATCH --partition=gp1d            

#SBATCH --nodes=2                   # 2 個節點
#SBATCH --gres=gpu:2                # 每task 2 張 GPU
#SBATCH --cpus-per-task=8           # 每個 client 分 4 顆 CPU
#SBATCH --output=z_fed_client.out
#SBATCH --error=z_fed_client.out

#module purge
#module load singularity

# 參數
SIF=/home/your_account/.sif 
SINGULARITY="singularity run --nv $SIF"

# defind master
MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_ADDR

## twcc 不知為何SLURM_GPUS_ON_NODE 為空
nvidia-smi --list-gpus
NGPU=$(nvidia-smi -L | wc -l) # $SLURM_GPUS_ON_NODE
export NGPU

#參數
NumRounds=3
EPOCHS=50
BATCH=32
WORKERS=8

echo "==============================="

cd /home/your account/yolov9

for ROUND in $(seq 0 $((NumRounds-1))); do
  echo "===== Round ${ROUND} START ====="
  
  # 1) 並行 4 個 client：宿主機用 srun 叫容器內的 run_client.sh
  for CLIENT in 0 1 2 3; do
    cmd="srun --gres=gpu:$NGPU --mpi=pmix \
              $SINGULARITY bash run_client.sh ${CLIENT} ${ROUND}"
    echo $cmd
    $cmd
  done
  wait
  echo "→ Round ${ROUND} local training done."

  # 2) 聚合 (只在宿主機呼叫一次)
  echo "→ Aggregating weights for Round ${ROUND}…"
    cmd="srun --gres=gpu:$NGPU --mpi=pmix \
              $SINGULARITY bash run_aggregate.sh ${ROUND}"
    echo $cmd
    $cmd
  echo "===== Round ${ROUND} COMPLETE → global_round_$((ROUND+1)).pt ====="
done

echo "=== All ${NumRounds} Rounds Finished ==="
